# API server

- query: irate(apiserver_request_total{verb="POST", resource="pods", subresource="binding",code="201"}[2m]) > 0
  metricName: schedulingThroughput

- query: histogram_quantile(0.99, sum(irate(apiserver_request_duration_seconds_bucket{apiserver="kube-apiserver", verb=~"LIST|GET", subresource!~"log|exec|portforward|attach|proxy"}[2m])) by (le, resource, verb, scope)) > 0
  metricName: readOnlyAPICallsLatency

- query: histogram_quantile(0.99, sum(irate(apiserver_request_duration_seconds_bucket{apiserver="kube-apiserver", verb=~"POST|PUT|DELETE|PATCH", subresource!~"log|exec|portforward|attach|proxy"}[2m])) by (le, resource, verb, scope)) > 0
  metricName: mutatingAPICallsLatency

- query: sum(irate(apiserver_request_total{apiserver="kube-apiserver",verb!="WATCH"}[2m])) by (verb,resource,code) > 0
  metricName: APIRequestRate

# Containers & pod metrics

- query: (sum(irate(container_cpu_usage_seconds_total{name!="",container!="POD",namespace=~"openshift-.*|cilium|stackrox|calico.*|tigera.*"}[2m]) * 100) by (container, pod, namespace, node) and on (node) kube_node_role{role="master"}) > 0
  metricName: containerCPU-Masters

- query: (avg(irate(container_cpu_usage_seconds_total{name!="",container!="POD",namespace=~"openshift-.*|cilium|stackrox|calico.*|tigera.*"}[2m]) * 100 and on (node) kube_node_role{role="worker"}) by (namespace, container)) > 0
  metricName: containerCPU-AggregatedWorkers

- query: (sum(irate(container_cpu_usage_seconds_total{name!="",container!="POD",namespace=~"openshift-(monitoring|sdn|ovn-kubernetes|multus|ingress)|stackrox"}[2m]) * 100) by (container, pod, namespace, node) and on (node) kube_node_role{role="infra"}) > 0
  metricName: containerCPU-Infra

- query: (sum(container_memory_rss{name!="",container!="POD",namespace=~"openshift-.*|cilium|stackrox|calico.*|tigera.*"}) by (container, pod, namespace, node) and on (node) kube_node_role{role="master"}) > 0
  metricName: containerMemory-Masters

- query: avg(container_memory_rss{name!="",container!="POD",namespace=~"openshift-.*|cilium|stackrox|calico.*|tigera.*"} and on (node) kube_node_role{role="worker"}) by (container, namespace)
  metricName: containerMemory-AggregatedWorkers

- query: (sum(container_memory_rss{name!="",container!="POD",namespace=~"openshift-.*|cilium|stackrox|calico.*|tigera.*"}) by (container, pod, namespace, node) and on (node) kube_node_role{role="infra"}) > 0
  metricName: containerMemory-Infra

# Node metrics: CPU & Memory

- query: (sum(irate(node_cpu_seconds_total[2m])) by (mode,instance) and on (instance) label_replace(kube_node_role{role="master"}, "instance", "$1", "node", "(.+)")) > 0
  metricName: nodeCPU-Masters

- query: (avg((sum(irate(node_cpu_seconds_total[2m])) by (mode,instance) and on (instance) label_replace(kube_node_role{role="worker"}, "instance", "$1", "node", "(.+)"))) by (mode)) > 0
  metricName: nodeCPU-AggregatedWorkers

- query: (sum(irate(node_cpu_seconds_total[2m])) by (mode,instance) and on (instance) label_replace(kube_node_role{role="infra"}, "instance", "$1", "node", "(.+)")) > 0
  metricName: nodeCPU-Infra

# We compute memory utilization by substrating available memory to the total
#
- query: avg((node_memory_MemTotal_bytes - node_memory_MemAvailable_bytes) and on (instance) label_replace(kube_node_role{role="worker"}, "instance", "$1", "node", "(.+)"))
  metricName: nodeMemoryUtilization-AggregatedWorkers

- query: (node_memory_MemTotal_bytes - node_memory_MemAvailable_bytes) and on (instance) label_replace(kube_node_role{role="master"}, "instance", "$1", "node", "(.+)")
  metricName: nodeMemoryUtilization-Masters

- query: (node_memory_MemTotal_bytes - node_memory_MemAvailable_bytes) and on (instance) label_replace(kube_node_role{role="infra"}, "instance", "$1", "node", "(.+)")
  metricName: nodeMemoryUtilization-Infra

# Kubelet & CRI-O runtime metrics

- query: irate(process_cpu_seconds_total{service="kubelet",job="kubelet"}[2m]) * 100 and on (node) topk(3,avg_over_time(irate(process_cpu_seconds_total{service="kubelet",job="kubelet"}[2m])[{{ .elapsed }}:]) and on (node) kube_node_role{role="worker"})
  metricName: kubeletCPU

- query: process_resident_memory_bytes{service="kubelet",job="kubelet"} and on (node) topk(3,max_over_time(irate(process_resident_memory_bytes{service="kubelet",job="kubelet"}[2m])[{{ .elapsed }}:]) and on (node) kube_node_role{role="worker"})
  metricName: kubeletMemory

- query: irate(process_cpu_seconds_total{service="kubelet",job="crio"}[2m]) * 100 and on (node) topk(3,avg_over_time(irate(process_cpu_seconds_total{service="kubelet",job="crio"}[2m])[{{ .elapsed }}:]) and on (node) kube_node_role{role="worker"})
  metricName: crioCPU

- query: process_resident_memory_bytes{service="kubelet",job="crio"} and on (node) topk(3,max_over_time(irate(process_resident_memory_bytes{service="kubelet",job="crio"}[2m])[{{ .elapsed }}:]) and on (node) kube_node_role{role="worker"})
  metricName: crioMemory

# Etcd metrics

- query: histogram_quantile(0.99, rate(etcd_disk_backend_commit_duration_seconds_bucket[2m]))
  metricName: 99thEtcdDiskBackendCommitDurationSeconds

- query: histogram_quantile(0.99, rate(etcd_disk_wal_fsync_duration_seconds_bucket[2m]))
  metricName: 99thEtcdDiskWalFsyncDurationSeconds

- query: histogram_quantile(0.99, rate(etcd_network_peer_round_trip_time_seconds_bucket[5m]))
  metricName: 99thEtcdRoundTripTimeSeconds

- query: sum by (cluster_version)(etcd_cluster_version)
  metricName: etcdVersion
  instant: true

# Cluster metrics

- query: sum(kube_namespace_status_phase) by (phase) > 0
  metricName: namespaceCount

- query: sum(kube_pod_status_phase{}) by (phase)
  metricName: podStatusCount

- query: count(kube_secret_info{})
  metricName: secretCount
  instant: true

- query: count(kube_deployment_spec_replicas{})
  metricName: deploymentCount
  instant: true

- query: count(kube_configmap_info{})
  metricName: configmapCount
  instant: true

- query: count(kube_service_info{})
  metricName: serviceCount
  instant: true

- query: count(openshift_route_created{})
  metricName: routeCount
  instant: true

- query: kube_node_role
  metricName: nodeRoles

- query: sum(kube_node_status_condition{status="true"}) by (condition)
  metricName: nodeStatus

- query: count(kube_replicaset_spec_replicas{})
  metricName: replicaSetCount
  instant: true

- query: count(kube_pod_info{} AND ON (pod) kube_pod_status_phase{phase="Running"}==1) by (node)
  metricName: podDistribution

# Prometheus metrics

- query: openshift:prometheus_tsdb_head_series:sum{job="prometheus-k8s"}
  metricName: prometheus-timeseriestotal

- query: openshift:prometheus_tsdb_head_samples_appended_total:sum{job="prometheus-k8s"}
  metricName: prometheus-ingestionrate

# Retain the raw CPU seconds totals for comparison
- query: sum( node_cpu_seconds_total and on (instance) label_replace(kube_node_role{role="worker",role!="infra"}, "instance", "$1", "node", "(.+)") ) by (mode)
  metricName: nodeCPUSeconds-Workers
  instant: true
  captureStart: true

- query: sum( node_cpu_seconds_total and on (instance) label_replace(kube_node_role{role="master"}, "instance", "$1", "node", "(.+)") ) by (mode)
  metricName: nodeCPUSeconds-Masters
  instant: true
  captureStart: true

- query: sum( node_cpu_seconds_total and on (instance) label_replace(kube_node_role{role="infra"}, "instance", "$1", "node", "(.+)") ) by (mode)
  metricName: nodeCPUSeconds-Infra
  instant: true
  captureStart: true

- query: sum (  container_cpu_usage_seconds_total {  id =~ "/system.slice|/system.slice/kubelet.service|/system.slice/ovs-vswitchd.service|/system.slice/crio.service|/kubepods.slice" } and on (node) kube_node_role{ role = "worker",role != "infra" } )  by   (   id  )
  metricName: cgroupCPUSeconds-Workers
  instant: true
  captureStart: true

- query: sum (  container_memory_rss {  id =~ "/system.slice|/system.slice/kubelet.service|/system.slice/ovs-vswitchd.service|/system.slice/crio.service|/kubepods.slice" } and on (node) kube_node_role{ role = "worker",role != "infra" } )  by   (   id  )
  metricName: cgroupMemoryRSS-Workers
  instant: true
  captureStart: true

- query: sum (  container_cpu_usage_seconds_total {  id =~ "/system.slice|/system.slice/kubelet.service|/system.slice/ovs-vswitchd.service|/system.slice/crio.service|/kubepods.slice" }  and on (node) kube_node_role{ role = "master" } )  by   (   id  )
  metricName: cgroupCPUSeconds-Masters
  instant: true
  captureStart: true

- query: sum (  container_memory_rss {  id =~ "/system.slice|/system.slice/kubelet.service|/system.slice/ovs-vswitchd.service|/system.slice/crio.service|/kubepods.slice" } and on (node) kube_node_role{ role = "master" } )  by   (   id  )
  metricName: cgroupMemoryRSS-Masters
  instant: true
  captureStart: true

- query: sum (  container_cpu_usage_seconds_total {  id =~ "/system.slice|/system.slice/kubelet.service|/system.slice/ovs-vswitchd.service|/system.slice/crio.service|/kubepods.slice" }  and on (node) kube_node_role{ role = "infra" } )  by   (   id  )
  metricName: cgroupCPUSeconds-Infra
  instant: true
  captureStart: true

- query: sum (  container_memory_rss {  id =~ "/system.slice|/system.slice/kubelet.service|/system.slice/ovs-vswitchd.service|/system.slice/crio.service|/kubepods.slice" } and on (node) kube_node_role{ role = "infra" } )  by   (   id  )
  metricName: cgroupMemoryRSS-Infra
  instant: true
  captureStart: true

- query: sum( container_cpu_usage_seconds_total{container!~"POD|",namespace=~"openshift-.*"} )  by (namespace)
  metricName: cgroupCPUSeconds-namespaces
  instant: true
  captureStart: true

- query: sum( container_memory_rss{container!~"POD|",namespace=~"openshift-.*"} )  by (namespace)
  metricName: cgroupMemoryRSS-namespaces
  instant: true
  captureStart: true

# OpenShift virtualization
# elapsed is the job duration time
## metadata
- query: kubevirt_hyperconverged_operator_health_status{}
  metricName: hco-operator-health

- query: kubevirt_hco_system_health_status{}
  metricName: hco-system-health

- query: sum(kubevirt_number_of_vms)
  metricName: total-vm-created
  instant: true

- query: sum(cnv:vmi_status_running:count)
  metricName: total-vmi-running
  instant: true

- query: cluster:vmi_request_cpu_cores:sum
  metricName: total-cpu-request
  instant: true

- query: sum(kubevirt_vm_resource_requests)
  metricName: total-mem-request
  instant: true

- query: sum(up{namespace="openshift-cnv",pod=~"virt-api-.*"}) or vector(0)
  metricName: virt-api-pod-count
  instant: true

- query: sum(kubevirt_memory_delta_from_requested_bytes)
  metricName: virt-infra-pod-total-mem-usage-request-delta
  instant: true

- query: sum(kubevirt_vmi_launcher_memory_overhead_bytes{}) / count (kubevirt_vmi_launcher_memory_overhead_bytes{})
  metricName: avg-virt-launcher-mem-overhead
  instant: true

## CPU and Memory related metircs
- query: avg(avg_over_time(rate(container_cpu_usage_seconds_total{name!="", namespace="openshift-cnv", container!="POD"}[2m])[{{.elapsed}}:])) by (container)
  metricName: avg-infra-pod-cpu-usage
  instant: true

- query: max(max_over_time(rate(container_cpu_usage_seconds_total{name!="", namespace="openshift-cnv", container!="POD"}[2m])[{{.elapsed}}:])) by (container)
  metricName: max-infra-pod-cpu-usage
  instant: true

- query: avg(avg_over_time(container_memory_rss{name!="", namespace="openshift-cnv", container!="POD"}[{{.elapsed}}:])) by (container)
  metricName: avg-infra-pod-memory-usage
  instant: true

- query: max(max_over_time(container_memory_rss{name!="", namespace="openshift-cnv", container!="POD"}[{{.elapsed}}:])) by (container)
  metricName: max-infra-pod-memory-usage
  instant: true

- query: avg(avg_over_time(kubevirt_memory_delta_from_requested_bytes[{{.elapsed}}:])) by (container)
  metricName: avg-infra-container-request-usage-delta
  instant: true

- query: max(max_over_time(kubevirt_memory_delta_from_requested_bytes[{{.elapsed}}:])) by (container)
  metricName: max-infra-container-request-usage-delta
  instant: true

- query: avg_over_time(label_replace(sum(container_memory_rss{pod=~"virt-launcher.*"}) by (pod), "vmi", "$1", "pod", "virt-launcher-(.*)-[^-]+$")[{{.elapsed}}:])
  metricName: avg-mem-usage-by-vmi
  instant: true

- query: avg_over_time(label_replace(sum(rate(container_cpu_usage_seconds_total{pod=~"virt-launcher.*"}[2m])) by (pod),"vmi", "$1", "pod", "virt-launcher-(.*)-[^-]+$")[{{.elapsed}}:])
  metricName: avg-cpu-usage-by-vmi
  instant: true

## storage related metrics
- query: avg(avg_over_time(rate(kubevirt_vmi_storage_iops_read_total[60s])[{{.elapsed}}:])) by (name)
  metricName: avg-vmi-read-iops
  instant: true

- query: avg(avg_over_time(rate(kubevirt_vmi_storage_iops_write_total[60s])[{{.elapsed}}:])) by (name)
  metricName: avg-vmi-write-iops
  instant: true

- query: avg(avg_over_time(rate(kubevirt_vmi_storage_read_traffic_bytes_total[60s])[{{.elapsed}}:])) by (name)
  metricName: avg-vmi-storage-read-traffic
  instant: true

- query: avg(avg_over_time((rate(kubevirt_vmi_storage_write_traffic_bytes_total[60s])[{{.elapsed}}:]))) by (name)
  metricName: avg-vmi-storage-write-traffic
  instant: true

## network related metrics
- query: avg(avg_over_time(rate(kubevirt_vmi_network_receive_bytes_total[60s])[{{.elapsed}}:])) by (name)
  metricName: avg-vmi-traffic-received
  instant: true

- query: avg(avg_over_time(rate(kubevirt_vmi_network_transmit_bytes_total[60s])[{{.elapsed}}:])) by (name)
  metricName: avg-vmi-traffic-transmitted
  instant: true

## Virt API reated metrics
- query: avg_over_time(sum(rate(apiserver_request_total{group=~".*kubevirt.*", verb=~"LIST|GET"}[1m]))[{{.elapsed}}:])
  metricName: avg-virt-api-read-request-rate
  instant: true

- query: avg_over_time(sum(rate(apiserver_request_total{group=~".*kubevirt.*", verb=~"POST|PUT|PATCH|DELETE"}[1m]))[{{.elapsed}}:])
  metricName: avg-virt-api-write-request-rate
  instant: true

- query: avg_over_time(sum(rate(apiserver_request_total{group=~".*kubevirt.*", code!~"2.."}[1m]))[{{.elapsed}}:])
  metricName: avg-virt-api-error-rate
  instant: true

## vmi migration reated metrics
- query: sum(kubevirt_vmi_migration_succeeded)
  metricName: migration-succeeded

- query: sum(kubevirt_vmi_migrations_in_scheduling_phase)
  metricName: migration-in-scheduling-phase

- query: sum(kubevirt_vmi_migrations_in_running_phase)
  metricName: migration-in-running-phase

- query: sum(kubevirt_vmi_migrations_in_pending_phase)
  metricName: migration-in-pending-phase
